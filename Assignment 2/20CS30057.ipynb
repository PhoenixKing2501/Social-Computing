{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from typing import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', download_dir='.env/nltk_data')\n",
    "\n",
    "TRAIN_PATH = \"absc_data/train.csv\"\n",
    "TEST_PATH = \"absc_data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the 5 most frequently occurring aspects in the training set for each category (`restaurant` and `laptop`) and visualize the data (category and frequency) using a bar graph (use matplotlib library). You should have 2 bar graphs (one for each category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for each category\n",
    "restaurant_aspects = train[train['Category'] == 'Restaurant']['Aspect Term']\n",
    "laptop_aspects = train[train['Category'] == 'Laptop']['Aspect Term']\n",
    "\n",
    "# Count the frequency of each aspect term\n",
    "restaurant_aspect_counts = Counter(restaurant_aspects)\n",
    "laptop_aspect_counts = Counter(laptop_aspects)\n",
    "\n",
    "# Get the 5 most common aspects for each category\n",
    "top_5_restaurant_aspects = restaurant_aspect_counts.most_common(5)\n",
    "top_5_laptop_aspects = laptop_aspect_counts.most_common(5)\n",
    "\n",
    "# Separate the aspect terms and their counts for plotting\n",
    "restaurant_aspects, restaurant_counts = zip(*top_5_restaurant_aspects)\n",
    "laptop_aspects, laptop_counts = zip(*top_5_laptop_aspects)\n",
    "\n",
    "# Plot the bar graphs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Restaurant aspects bar graph\n",
    "axes[0].bar(restaurant_aspects, restaurant_counts, color='blue')\n",
    "axes[0].set_title('Top 5 Restaurant Aspects')\n",
    "axes[0].set_xlabel('Aspect Term')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# add counts over the bars\n",
    "for i, count in enumerate(restaurant_counts):\n",
    "    axes[0].text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Laptop aspects bar graph\n",
    "axes[1].bar(laptop_aspects, laptop_counts, color='green')\n",
    "axes[1].set_title('Top 5 Laptop Aspects')\n",
    "axes[1].set_xlabel('Aspect Term')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# add counts over the bars\n",
    "for i, count in enumerate(laptop_counts):\n",
    "    axes[1].text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the distribution of the number of aspects per sentence (the same sentence can contain multiple aspects) in the entire training set and visualize the data using a bar graph. You can check for sentences having 1, 2, 3 and more than 3 aspects. You should have a single bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sentence and count the number of aspects per sentence\n",
    "aspect_counts_per_sentence = train.groupby('Sentence')['Aspect Term'].count()\n",
    "\n",
    "# Categorize the counts\n",
    "aspect_distribution = aspect_counts_per_sentence.apply(\n",
    "    lambda x: '1' if x == 1 else ('2' if x == 2 else ('3' if x == 3 else 'More than 3')))\n",
    "\n",
    "# Count the frequency of each category\n",
    "aspect_distribution_counts = aspect_distribution.value_counts().sort_index()\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(aspect_distribution_counts.index,\n",
    "        aspect_distribution_counts.values, color='purple')\n",
    "plt.title('Distribution of Number of Aspects per Sentence')\n",
    "plt.xlabel('Number of Aspects')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# add counts over the bars\n",
    "for i, count in enumerate(aspect_distribution_counts):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Support Vector Machine (SVM) based sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "def concatenate_aspect_sentence(row: pd.DataFrame) -> str:\n",
    "    return f\"{row['Aspect Term']} [SEP] {row['Sentence']}\"\n",
    "\n",
    "\n",
    "train['Input'] = train.apply(concatenate_aspect_sentence, axis=1)\n",
    "test['Input'] = test.apply(concatenate_aspect_sentence, axis=1)\n",
    "\n",
    "X_train = train['Input']\n",
    "y_train = train['Polarity']\n",
    "X_test = test['Input']\n",
    "y_test = test['Polarity']\n",
    "\n",
    "# Model Creation\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        stop_words=stopwords.words('english'),\n",
    "        max_features=5000,\n",
    "    ),\n",
    "    SVC(),\n",
    ")\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "param_grid = {\n",
    "    'svc__C': [i*j\n",
    "               for i in [0.1, 1, 10]\n",
    "               for j in [1, 3]\n",
    "               ],\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Model Training\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Model Parameters:\")\n",
    "pp(grid_search.best_params_)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# print confusion matrix\n",
    "cm_disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. BERT based classifier (bert base uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

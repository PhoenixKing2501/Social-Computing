{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "from typing import Counter\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# nltk.download('stopwords', download_dir='.env/nltk_data')\n",
    "\n",
    "TRAIN_PATH = \"absc_data/train.csv\"\n",
    "TEST_PATH = \"absc_data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the 5 most frequently occurring aspects in the training set for each category (`restaurant` and `laptop`) and visualize the data (category and frequency) using a bar graph (use matplotlib library). You should have 2 bar graphs (one for each category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for each category\n",
    "restaurant_aspects = train[train['Category'] == 'Restaurant']['Aspect Term']\n",
    "laptop_aspects = train[train['Category'] == 'Laptop']['Aspect Term']\n",
    "\n",
    "# Count the frequency of each aspect term\n",
    "restaurant_aspect_counts = Counter(restaurant_aspects)\n",
    "laptop_aspect_counts = Counter(laptop_aspects)\n",
    "\n",
    "# Get the 5 most common aspects for each category\n",
    "top_5_restaurant_aspects = restaurant_aspect_counts.most_common(5)\n",
    "top_5_laptop_aspects = laptop_aspect_counts.most_common(5)\n",
    "\n",
    "# Separate the aspect terms and their counts for plotting\n",
    "restaurant_aspects, restaurant_counts = zip(*top_5_restaurant_aspects)\n",
    "laptop_aspects, laptop_counts = zip(*top_5_laptop_aspects)\n",
    "\n",
    "# Plot the bar graphs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Restaurant aspects bar graph\n",
    "axes[0].bar(restaurant_aspects, restaurant_counts, color='blue')\n",
    "axes[0].set_title('Top 5 Restaurant Aspects')\n",
    "axes[0].set_xlabel('Aspect Term')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# add counts over the bars\n",
    "for i, count in enumerate(restaurant_counts):\n",
    "    axes[0].text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Laptop aspects bar graph\n",
    "axes[1].bar(laptop_aspects, laptop_counts, color='green')\n",
    "axes[1].set_title('Top 5 Laptop Aspects')\n",
    "axes[1].set_xlabel('Aspect Term')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# add counts over the bars\n",
    "for i, count in enumerate(laptop_counts):\n",
    "    axes[1].text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the distribution of the number of aspects per sentence (the same sentence can contain multiple aspects) in the entire training set and visualize the data using a bar graph. You can check for sentences having 1, 2, 3 and more than 3 aspects. You should have a single bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sentence and count the number of aspects per sentence\n",
    "aspect_counts_per_sentence = train.groupby('Sentence')['Aspect Term'].count()\n",
    "\n",
    "# Categorize the counts\n",
    "aspect_distribution = aspect_counts_per_sentence.apply(\n",
    "    lambda x: '1' if x == 1 else ('2' if x == 2 else ('3' if x == 3 else 'More than 3')))\n",
    "\n",
    "# Count the frequency of each category\n",
    "aspect_distribution_counts = aspect_distribution.value_counts().sort_index()\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(aspect_distribution_counts.index,\n",
    "        aspect_distribution_counts.values, color='purple')\n",
    "plt.title('Distribution of Number of Aspects per Sentence')\n",
    "plt.xlabel('Number of Aspects')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# add counts over the bars\n",
    "for i, count in enumerate(aspect_distribution_counts):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Support Vector Machine (SVM) based sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def concatenate_aspect_sentence(row: pd.DataFrame) -> str:\n",
    "    return f\"{row['Aspect Term']} [SEP] {row['Sentence']}\"\n",
    "\n",
    "\n",
    "X_train = train.apply(concatenate_aspect_sentence, axis=1)\n",
    "X_test = test.apply(concatenate_aspect_sentence, axis=1)\n",
    "\n",
    "y_train = train['Polarity']\n",
    "y_test = test['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Model Creation\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        stop_words=stopwords.words('english'),\n",
    "        max_features=5000,\n",
    "    ),\n",
    "    SVC(),\n",
    ")\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "param_grid = {\n",
    "    'svc__C': [i*j\n",
    "               for i in [0.1, 1, 10]\n",
    "               for j in [1, 3]\n",
    "               ],\n",
    "    'svc__kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5,\n",
    "                           scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Model Training\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Model Parameters:\")\n",
    "pp(grid_search.best_params_)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# save the model\n",
    "joblib.dump(best_model, 'svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# load the model\n",
    "best_model = joblib.load('svm_model.joblib')\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test, digits=5))\n",
    "\n",
    "# print confusion matrix\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. BERT based classifier (bert base uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CLASSES = train['Polarity'].unique()\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "CLASS2INDEX = {c: i for i, c in enumerate(CLASSES)}\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, *, max_len=128):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    # END __init__\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = f\"{row['Aspect Term']} [SEP] {row['Sentence']}\"\n",
    "        label = NUM_CLASSES * [0]\n",
    "        label[CLASS2INDEX[row['Polarity']]] = 1\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "    # END __getitem__\n",
    "# END BertDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(model_name)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    # END __init__\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "        )\n",
    "        return self.out(output)\n",
    "    # END forward\n",
    "# END BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = BertDataset(train, tokenizer, max_len=MAX_LEN)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataset = BertDataset(val, tokenizer, max_len=MAX_LEN)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "SAVE_PATH = \"bert_model.pth\"\n",
    "\n",
    "model = BertModel(\"bert-base-uncased\", NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(SAVE_PATH, weights_only=True))\n",
    "\n",
    "model = model.to(DEVICE, non_blocking=True)\n",
    "for bert_param in model.bert.parameters():\n",
    "    bert_param.requires_grad = False\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "for epoch in trange(EPOCHS):\n",
    "    tqdm.write(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    tqdm.write(\"Training...\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(train_loader),\n",
    "                         leave=False,\n",
    "                         total=len(train_loader),\n",
    "                         colour='magenta'):\n",
    "        input_ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE, non_blocking=True)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, non_blocking=True)\n",
    "        label = batch['label'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        label = label.type_as(output)\n",
    "        loss = loss_fn(output, label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        correct += torch.sum(pred == torch.argmax(label, dim=1)).item()\n",
    "    # END for i, batch in enumerate(train_loader)\n",
    "\n",
    "    tqdm.write(f\"Training Loss:            {train_loss / len(train)}\")\n",
    "    tqdm.write(f\"Training Accuracy:        {correct / len(train)}\")\n",
    "\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    tqdm.write(\"\\nValidating...\")\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        for batch in tqdm(val_loader, leave=False, colour='green'):\n",
    "            input_ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(\n",
    "                DEVICE, non_blocking=True)\n",
    "            token_type_ids = batch['token_type_ids'].to(\n",
    "                DEVICE, non_blocking=True)\n",
    "            label = batch['label'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            label = label.type_as(output)\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct += torch.sum(pred == torch.argmax(label, dim=1)).item()\n",
    "        # END for batch in val_loader\n",
    "    # END with torch.inference_mode()\n",
    "\n",
    "    val_acc = correct / len(val)\n",
    "\n",
    "    tqdm.write(f\"Validation Loss:          {val_loss / len(val)}\")\n",
    "    tqdm.write(f\"Validation Accuracy:      {val_acc}\")\n",
    "    tqdm.write(f\"Best Validation Accuracy: {best_acc}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        tqdm.write(\"Saving model...\\n\")\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "    # END if val_loss < best_loss\n",
    "# END for epoch in trange(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "test_dataset = BertDataset(test, tokenizer, max_len=MAX_LEN)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(\"bert-base-uncased\", NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(SAVE_PATH, weights_only=True))\n",
    "model = model.to(DEVICE, non_blocking=True)\n",
    "model.eval()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "correct = []\n",
    "preds = []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, colour='cyan'):\n",
    "        input_ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE, non_blocking=True)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, non_blocking=True)\n",
    "        label = batch['label'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        label = label.type_as(output)\n",
    "        loss = loss_fn(output, label)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        corr = torch.argmax(label, dim=1).numpy(force=True)\n",
    "        pred = torch.argmax(output, dim=1).numpy(force=True)\n",
    "\n",
    "        correct.extend(corr)\n",
    "        preds.extend(pred)\n",
    "    # END for batch in test_loader\n",
    "# END with torch.inference_mode()\n",
    "\n",
    "tqdm.write(f\"Test Loss: {test_loss / len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "print(classification_report(correct, preds, target_names=CLASSES, zero_division=0, digits=5))\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(\n",
    "    correct, preds, display_labels=CLASSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "TRAIN_PATH = \"absc_data/train.csv\"\n",
    "TEST_PATH = \"absc_data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3630 1210 1211\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "CLASSES = train['Polarity'].unique()\n",
    "CLASS2INDEX = {c: i for i, c in enumerate(CLASSES)}\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, *, max_len=128):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    # END __init__\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = f\"{row['Aspect Term']} [SEP] {row['Sentence']}\"\n",
    "        label = NUM_CLASSES * [0]\n",
    "        label[CLASS2INDEX[row['Polarity']]] = 1\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "    # END __getitem__\n",
    "# END BertDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(model_name)\n",
    "        # self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    # END __init__\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False\n",
    "        )\n",
    "        # output = self.drop(output)\n",
    "        return self.out(output)\n",
    "    # END forward\n",
    "# END BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = BertDataset(train, tokenizer, max_len=MAX_LEN)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_dataset = BertDataset(val, tokenizer, max_len=MAX_LEN)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "model = BertModel(\"bert-base-uncased\", NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "\n",
    "model = model.to(DEVICE, non_blocking=True)\n",
    "for bert_param in model.bert.parameters():\n",
    "    bert_param.requires_grad = False\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22519bc8ce894f9e9db026bb514832dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2260d5fda389440384fe341a24417f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.93863093852997\n",
      "Training Accuracy:        0.6586776859504132\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb83dd815e94325970f3500ad41d2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:          8.109703063964844\n",
      "Validation Accuracy:      0.6925619834710743\n",
      "Best Validation Accuracy: 0\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 2/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9589e4b1a01a4ff2ae19b8e6c9c085e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.483178079128265\n",
      "Training Accuracy:        0.6650137741046832\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93602f87d10a4fdaaa655539d6ef0d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:          8.129157066345215\n",
      "Validation Accuracy:      0.687603305785124\n",
      "Best Validation Accuracy: 0.6925619834710743\n",
      "\n",
      "Epoch 3/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6317275e645747b08b121ac81afef6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.359801590442657\n",
      "Training Accuracy:        0.6672176308539944\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f148834e341c40408023b30f0d9e44c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:          8.055902183055878\n",
      "Validation Accuracy:      0.6917355371900826\n",
      "Best Validation Accuracy: 0.6925619834710743\n",
      "\n",
      "Epoch 4/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b12fdfd47942a599e40365d8ec3fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.39401662349701\n",
      "Training Accuracy:        0.6584022038567493\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ce48ddec194247873f309bc392e7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:          8.091739416122437\n",
      "Validation Accuracy:      0.6958677685950413\n",
      "Best Validation Accuracy: 0.6925619834710743\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 5/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c9dfbbedca4f329f504e87a1d43a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.50907838344574\n",
      "Training Accuracy:        0.6647382920110193\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6162c59b51e49faa1151417886b36ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:          8.174599528312683\n",
      "Validation Accuracy:      0.6834710743801653\n",
      "Best Validation Accuracy: 0.6958677685950413\n",
      "\n",
      "Epoch 6/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a43973645048a28ed545c2b20a54ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.79318594932556\n",
      "Training Accuracy:        0.6556473829201102\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c282fb39184e599a362e1436a9a857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:          7.996895968914032\n",
      "Validation Accuracy:      0.7016528925619835\n",
      "Best Validation Accuracy: 0.6958677685950413\n",
      "Saving model...\n",
      "\n",
      "\n",
      "Epoch 7/30\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7bd04c7a3e4a92ae6404bf5749a7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:            24.405308425426483\n",
      "Training Accuracy:        0.6680440771349863\n",
      "\n",
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfb4ce8e427488fb3a2144a4eb33016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for epoch in trange(EPOCHS):\n",
    "    tqdm.write(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    tqdm.write(\"Training...\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(train_loader),\n",
    "                         leave=False,\n",
    "                         total=len(train_loader),\n",
    "                         colour='magenta'):\n",
    "        input_ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE, non_blocking=True)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, non_blocking=True)\n",
    "        label = batch['label'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        label = label.type_as(output)\n",
    "        loss = loss_fn(output, label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        correct += torch.sum(pred == torch.argmax(label, dim=1)).item()\n",
    "    # END for i, batch in enumerate(train_loader)\n",
    "\n",
    "    tqdm.write(f\"Training Loss:            {train_loss}\")\n",
    "    tqdm.write(f\"Training Accuracy:        {correct / len(train)}\")\n",
    "\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    tqdm.write(\"\\nValidating...\")\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        for batch in tqdm(val_loader, leave=False, colour='green'):\n",
    "            input_ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(\n",
    "                DEVICE, non_blocking=True)\n",
    "            token_type_ids = batch['token_type_ids'].to(\n",
    "                DEVICE, non_blocking=True)\n",
    "            label = batch['label'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            label = label.type_as(output)\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct += torch.sum(pred == torch.argmax(label, dim=1)).item()\n",
    "        # END for batch in val_loader\n",
    "    # END with torch.inference_mode()\n",
    "\n",
    "    val_acc = correct / len(val)\n",
    "\n",
    "    tqdm.write(f\"Validation Loss:          {val_loss}\")\n",
    "    tqdm.write(f\"Validation Accuracy:      {val_acc}\")\n",
    "    tqdm.write(f\"Best Validation Accuracy: {best_acc}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        tqdm.write(\"Saving model...\\n\")\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "    # END if val_loss < best_loss\n",
    "# END for epoch in trange(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "test_dataset = BertDataset(test, tokenizer, max_len=MAX_LEN)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "model = BertModel(\"bert-base-uncased\", NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model = model.to(DEVICE, non_blocking=True)\n",
    "model.eval()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "correct = []\n",
    "preds = []\n",
    "test_loss = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_loader, colour='cyan'):\n",
    "        input_ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE, non_blocking=True)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, non_blocking=True)\n",
    "        label = batch['label'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        label = label.type_as(output)\n",
    "        loss = loss_fn(output, label)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        corr = torch.argmax(label, dim=1).numpy(force=True)\n",
    "        pred = torch.argmax(output, dim=1).numpy(force=True)\n",
    "\n",
    "        correct.extend(corr)\n",
    "        preds.extend(pred)\n",
    "        # correct += torch.sum(pred == torch.argmax(label, dim=1)).item()\n",
    "    # END for batch in test_loader\n",
    "# END with torch.inference_mode()\n",
    "\n",
    "tqdm.write(f\"Test Loss:     {test_loss}\")\n",
    "# tqdm.write(f\"Test Accuracy: {correct / len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(correct, preds, target_names=CLASSES, zero_division=0))\n",
    "cm_display = ConfusionMatrixDisplay.from_predictions(\n",
    "    correct, preds, display_labels=CLASSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
